{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers.core import Dense, Activation, Reshape\n",
    "from keras.layers import Conv2D, LSTM\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"game/\")\n",
    "import wrapped_flappy_bird as game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "game_state = game.GameState()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_rows , img_cols = 80, 80\n",
    "#Convert image into Black and white\n",
    "img_channels = 4 #We stack 4 frames\n",
    "from collections import deque\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.action_size = 2\n",
    "\n",
    "        # These are hyper parameters for the DQN\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.0001\n",
    "        self.epsilon = 0.1\n",
    "        self.epsilon_min = 0.0001\n",
    "        self.batch_size = 32\n",
    "        self.train_start = 10000\n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=200000)\n",
    "        self.train_buffer = deque(maxlen=10240)\n",
    "        self.model = self.build_model()\n",
    "\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    # state is input and Q Value of each action is output of network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, kernel_size=(8,8), strides=(4, 4), activation='relu',input_shape=(img_rows,img_cols,img_channels),kernel_initializer=initializers.random_normal(stddev=0.01)))  #80*80*4\n",
    "        model.add(Conv2D(64, kernel_size=(4,4), strides=(2, 2), activation='relu',kernel_initializer=initializers.random_normal(stddev=0.01)))\n",
    "        model.add(Conv2D(64, kernel_size=(3,3), strides=(1, 1), activation='relu',kernel_initializer=initializers.random_normal(stddev=0.01)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512,activation='relu',kernel_initializer=initializers.random_normal(stddev=0.01)))\n",
    "        model.add(Dense(self.action_size,activation='linear',kernel_initializer=initializers.random_normal(stddev=0.01)))\n",
    "        model.summary()\n",
    "        model.compile(loss='mse', optimizer=optimizers.Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    # get action from model using epsilon-greedy policy\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            state = state.reshape(1, state.shape[0], state.shape[1], state.shape[2])\n",
    "            q_value = self.model.predict(state)\n",
    "            return np.argmax(q_value[0])\n",
    "\n",
    "    # save sample <s,a,r,s'> to the replay memory\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    # pick samples randomly from replay memory (with batch_size)\n",
    "    def train_model(self):\n",
    "\n",
    "        #do training if buffer is full\n",
    "        if len(self.train_buffer) == self.train_buffer.maxlen:\n",
    "            batch_size = self.train_buffer.maxlen\n",
    "            states = np.zeros((batch_size, img_rows, img_cols, img_channels))\n",
    "            next_states = np.zeros((batch_size, img_rows, img_cols, img_channels))\n",
    "            target = np.zeros((batch_size, self.action_size)) \n",
    "            action, reward, done = [], [], []\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                states[i] = self.train_buffer[i][0]\n",
    "                action.append(self.train_buffer[i][1])\n",
    "                reward.append(self.train_buffer[i][2])\n",
    "                next_states[i] = self.train_buffer[i][3]\n",
    "                done.append(self.train_buffer[i][4])\n",
    "\n",
    "            target = self.model.predict(states)            \n",
    "            target_next = self.model.predict(next_states)\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                # Q Learning: get maximum Q value at s' from target model\n",
    "                if done[i]:\n",
    "                    target[i][action[i]] = reward[i]\n",
    "                else:\n",
    "                    a = np.argmax(target_next[i])\n",
    "                    target[i][action[i]] = reward[i] + self.discount_factor * (\n",
    "                        target_next[i][a])\n",
    "                    \n",
    "            self.model.fit(states, target, 100, epochs=1, verbose=0)\n",
    "            self.train_buffer.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_state():\n",
    "    do_nothing = np.zeros(2)\n",
    "    do_nothing[random.randrange(2)] = 1\n",
    "    x_t, r_0, terminal = game_state.frame_step(do_nothing)\n",
    "    x_t = cv2.cvtColor(cv2.resize(x_t, (80, 80)), cv2.COLOR_BGR2GRAY)\n",
    "    ret, x_t = cv2.threshold(x_t,1,255,cv2.THRESH_BINARY)\n",
    "    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)\n",
    "    return s_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent = DQNAgent()\n",
    "scores, episodes = [], []\n",
    "action_count = 0\n",
    "ACTIONS = 2\n",
    "EXPLORE = 300000\n",
    "num_steps = 0\n",
    "\n",
    "EPISODES = 500000\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "    state = reset_state() \n",
    "        \n",
    "    while not done:\n",
    "        action = np.zeros([ACTIONS])\n",
    "        action_idx = agent.get_action(state)\n",
    "        action[action_idx] = 1\n",
    "        next_img, reward, done = game_state.frame_step(action)\n",
    "        \n",
    "        #process next image\n",
    "        next_img = cv2.cvtColor(cv2.resize(next_img, (80, 80)), cv2.COLOR_BGR2GRAY)\n",
    "        ret, next_img = cv2.threshold(next_img, 1, 255, cv2.THRESH_BINARY)\n",
    "        next_img = next_img.reshape(img_rows, img_cols, 1) #80x80x1\n",
    "        next_state = np.append(next_img,state[:, :, :3],axis=2)\n",
    "        num_steps += 1\n",
    "        agent.append_sample(state, action_idx, reward, next_state, done)\n",
    "        \n",
    "        if len(agent.memory) > agent.train_start: \n",
    "            if agent.epsilon > agent.epsilon_min:\n",
    "                agent.epsilon -= (agent.epsilon - agent.epsilon_min) / EXPLORE\n",
    "            \n",
    "            action_count += 1\n",
    "        \n",
    "            if action_count == 4:\n",
    "                mini_batch = random.sample(agent.memory, agent.batch_size)\n",
    "                agent.train_buffer.extend(mini_batch)\n",
    "                action_count = 0\n",
    "                          \n",
    "        agent.train_model()        \n",
    "        score += reward\n",
    "        state = next_state\n",
    "\n",
    "        \n",
    "        if done:\n",
    "            scores.append(score)\n",
    "            episodes.append(e)\n",
    "            print(\"episode:\", e, \" score:\", score, \" epsilon:\", agent.epsilon,\" memory size: \", len(agent.memory),\"number of steps: \", num_steps)\n",
    "\n",
    "    if e % 1000:\n",
    "        agent.model.save('flappy_bird_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " score: 63.50000000000047\n",
      " score: 1254.899999999963\n",
      " score: 896.1000000001061\n",
      " score: 35.900000000000205\n",
      " score: 1397.3999999998616\n",
      " score: 468.20000000003\n",
      " score: 445.30000000002593\n",
      " score: 569.500000000048\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-78b455635643>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0maction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction_idx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mnext_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgame_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframe_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m#process next image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Keras-FlappyBird-master\\game\\wrapped_flappy_bird.py\u001b[0m in \u001b[0;36mframe_step\u001b[1;34m(self, input_actions)\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[0mpygame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[1;31m#print (\"FPS\" , FPSCLOCK.get_fps())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m         \u001b[0mFPSCLOCK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFPS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m         \u001b[1;31m#print self.upperPipes[0]['y'] + PIPE_HEIGHT - int(BASEY * 0.2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimage_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#test model\n",
    "from keras.models import load_model\n",
    "model = load_model('flappy_bird__lstm_model.h5')\n",
    "ACTIONS = 2\n",
    "img_rows , img_cols = 80, 80\n",
    "img_channels = 4\n",
    "\n",
    "EPISODES = 100\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "    state = reset_state() \n",
    "        \n",
    "    while not done:\n",
    "        action = np.zeros([ACTIONS])\n",
    "        state_tmp = state.reshape(1, state.shape[0], state.shape[1], state.shape[2])\n",
    "        q_value = model.predict(state_tmp)\n",
    "        action_idx = np.argmax(q_value[0])\n",
    "        \n",
    "        action[action_idx] = 1\n",
    "        next_img, reward, done = game_state.frame_step(action)\n",
    "        \n",
    "        #process next image\n",
    "        next_img = cv2.cvtColor(cv2.resize(next_img, (80, 80)), cv2.COLOR_BGR2GRAY)\n",
    "        ret, next_img = cv2.threshold(next_img, 1, 255, cv2.THRESH_BINARY)\n",
    "        next_img = next_img.reshape(img_rows, img_cols, 1) #80x80x1\n",
    "        next_state = np.append(next_img,state[:, :, :3],axis=2)\n",
    "        score += reward\n",
    "        state = next_state\n",
    "\n",
    "        \n",
    "        if done:\n",
    "            print( \" score:\", score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
